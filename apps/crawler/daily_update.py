#!/usr/bin/env python3
"""
æ¯æ—¥ç§Ÿå±‹è³‡æ–™æ›´æ–°è…³æœ¬ - è»Ÿåˆªé™¤ç­–ç•¥
é©ç”¨æ–¼ cron job è‡ªå‹•åŸ·è¡Œ

ä½¿ç”¨æ–¹æ³•:
  python daily_update.py [--test] [--cleanup-days=7] [--batch-size=1000]
"""

import os
import sys
import json
import argparse
import logging
import requests
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# å°å…¥çˆ¬èŸ²æ¨¡çµ„
from crawler import crawl_5168_all_regions

# é…ç½®æ—¥èªŒ
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

# è¨­ç½®æ—¥èªŒæ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{LOG_DIR}/daily_update_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class DailyUpdateManager:
    def __init__(self, backend_url: str = "http://localhost:8000", test_mode: bool = False):
        self.backend_url = backend_url.rstrip('/')
        self.test_mode = test_mode
        self.start_time = datetime.now()
        self.stats = {
            'crawl_success': False,
            'crawl_count': 0,
            'import_success': False,
            'import_stats': {},
            'cleanup_success': False,
            'cleanup_stats': {},
            'errors': []
        }

    def run_daily_update(self, cleanup_days: int = 7, batch_size: int = 1000) -> Dict[str, Any]:
        """åŸ·è¡Œæ¯æ—¥æ›´æ–°æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹æ¯æ—¥ç§Ÿå±‹è³‡æ–™æ›´æ–°")
        logger.info(f"æ¨¡å¼: {'æ¸¬è©¦' if self.test_mode else 'æ­£å¼'}")
        
        try:
            # æ­¥é©Ÿ 1: çˆ¬å–æœ€æ–°è³‡æ–™
            self._step_crawl_data()
            
            # æ­¥é©Ÿ 2: æ‰¹æ¬¡æ›´æ–°è³‡æ–™åº«
            self._step_batch_update(batch_size)
            
            # æ­¥é©Ÿ 3: æ¸…ç†éæœŸè³‡æ–™
            if not self.test_mode:
                self._step_cleanup_old_data(cleanup_days)
            
            # æ­¥é©Ÿ 4: ç”Ÿæˆå ±å‘Š
            return self._generate_report()
            
        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥æ›´æ–°å¤±æ•—: {e}")
            self.stats['errors'].append(str(e))
            return self._generate_report()

    def _step_crawl_data(self):
        """æ­¥é©Ÿ 1: çˆ¬å–è³‡æ–™"""
        logger.info("ğŸ“¡ æ­¥é©Ÿ 1: é–‹å§‹çˆ¬å–æœ€æ–°ç§Ÿå±‹è³‡æ–™")
        
        try:
            if self.test_mode:
                logger.info("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šè·³éå¯¦éš›çˆ¬å–")
                self.stats['crawl_success'] = True
                self.stats['crawl_count'] = 0
                return
            
            # åŸ·è¡Œçˆ¬èŸ²
            result = crawl_5168_all_regions()
            
            # æª¢æŸ¥çˆ¬å–çµæœ
            if os.path.exists('data/houseprice_all_data.json'):
                with open('data/houseprice_all_data.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.stats['crawl_count'] = len(data)
                    
                logger.info(f"âœ… çˆ¬å–å®Œæˆï¼Œå…± {self.stats['crawl_count']} ç­†è³‡æ–™")
                self.stats['crawl_success'] = True
            else:
                raise Exception("çˆ¬å–å®Œæˆä½†æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ")
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±æ•—: {e}")
            self.stats['errors'].append(f"çˆ¬å–å¤±æ•—: {e}")
            raise

    def _step_batch_update(self, batch_size: int):
        """æ­¥é©Ÿ 2: æ‰¹æ¬¡æ›´æ–°è³‡æ–™åº«"""
        logger.info("ğŸ”„ æ­¥é©Ÿ 2: æ‰¹æ¬¡æ›´æ–°è³‡æ–™åº«")
        
        try:
            if self.test_mode:
                logger.info("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šæ¨¡æ“¬è³‡æ–™åº«æ›´æ–°")
                self.stats['import_success'] = True
                self.stats['import_stats'] = {
                    'imported': 100,
                    'updated': 50,
                    'skipped': 5,
                    'strategy': 'test-mode'
                }
                return
            
            # å‘¼å«å¾Œç«¯ API é€²è¡Œæ‰¹æ¬¡æ›´æ–°
            data_file = 'data/houseprice_all_data.json'
            
            with open(data_file, 'rb') as f:
                files = {'file': f}
                response = requests.post(
                    f'{self.backend_url}/api/import/batch-update',
                    files=files,
                    timeout=300  # 5åˆ†é˜è¶…æ™‚
                )
            
            if response.status_code == 200:
                self.stats['import_stats'] = response.json()
                self.stats['import_success'] = True
                logger.info(f"âœ… æ‰¹æ¬¡æ›´æ–°å®Œæˆ: {self.stats['import_stats']}")
            else:
                raise Exception(f"æ‰¹æ¬¡æ›´æ–°å¤±æ•—: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡æ›´æ–°å¤±æ•—: {e}")
            self.stats['errors'].append(f"æ‰¹æ¬¡æ›´æ–°å¤±æ•—: {e}")
            raise

    def _step_cleanup_old_data(self, cleanup_days: int):
        """æ­¥é©Ÿ 3: æ¸…ç†éæœŸè³‡æ–™"""
        logger.info(f"ğŸ§¹ æ­¥é©Ÿ 3: æ¸…ç† {cleanup_days} å¤©å‰çš„éæœŸè³‡æ–™")
        
        try:
            response = requests.post(
                f'{self.backend_url}/api/import/cleanup',
                json={'keepDays': cleanup_days, 'smart': True},
                timeout=60
            )
            
            if response.status_code == 200:
                self.stats['cleanup_stats'] = response.json()
                self.stats['cleanup_success'] = True
                logger.info(f"âœ… æ¸…ç†å®Œæˆ: {self.stats['cleanup_stats']}")
            else:
                raise Exception(f"æ¸…ç†å¤±æ•—: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å¤±æ•—: {e}")
            self.stats['errors'].append(f"æ¸…ç†å¤±æ•—: {e}")
            # æ¸…ç†å¤±æ•—ä¸é˜»æ“‹æ•´å€‹æµç¨‹

    def _generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ›´æ–°å ±å‘Š"""
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        success = (
            self.stats['crawl_success'] and 
            self.stats['import_success']
        )
        
        report = {
            'timestamp': end_time.isoformat(),
            'duration_seconds': duration,
            'success': success,
            'test_mode': self.test_mode,
            'stats': self.stats,
            'summary': self._create_summary()
        }
        
        # è¨˜éŒ„å ±å‘Š
        logger.info("ğŸ“Š æ›´æ–°å ±å‘Š:")
        logger.info(f"   æˆåŠŸ: {'âœ…' if success else 'âŒ'}")
        logger.info(f"   è€—æ™‚: {duration:.1f} ç§’")
        logger.info(f"   çˆ¬å–: {self.stats['crawl_count']} ç­†")
        if self.stats.get('import_stats'):
            import_stats = self.stats['import_stats']
            logger.info(f"   å°å…¥: {import_stats.get('imported', 0)} ç­†")
            logger.info(f"   æ›´æ–°: {import_stats.get('updated', 0)} ç­†")
        
        if self.stats['errors']:
            logger.warning(f"   éŒ¯èª¤: {len(self.stats['errors'])} å€‹")
            for error in self.stats['errors']:
                logger.warning(f"     - {error}")
        
        return report

    def _create_summary(self) -> str:
        """å‰µå»ºæ–‡å­—æ‘˜è¦"""
        if self.stats['crawl_success'] and self.stats['import_success']:
            return f"âœ… æ¯æ—¥æ›´æ–°æˆåŠŸå®Œæˆï¼Œè™•ç†äº† {self.stats['crawl_count']} ç­†è³‡æ–™"
        else:
            return f"âŒ æ¯æ—¥æ›´æ–°å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ"

def main():
    parser = argparse.ArgumentParser(description='æ¯æ—¥ç§Ÿå±‹è³‡æ–™æ›´æ–°è…³æœ¬')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼ˆä¸åŸ·è¡Œå¯¦éš›æ“ä½œï¼‰')
    parser.add_argument('--cleanup-days', type=int, default=7, help='ä¿ç•™å¹¾å¤©çš„èˆŠè³‡æ–™ï¼ˆé è¨­7å¤©ï¼‰')
    parser.add_argument('--batch-size', type=int, default=1000, help='æ‰¹æ¬¡è™•ç†å¤§å°ï¼ˆé è¨­1000ï¼‰')
    parser.add_argument('--backend-url', default='http://localhost:8000', help='å¾Œç«¯APIç¶²å€')
    
    args = parser.parse_args()
    
    # å‰µå»ºæ›´æ–°ç®¡ç†å™¨
    manager = DailyUpdateManager(
        backend_url=args.backend_url,
        test_mode=args.test
    )
    
    # åŸ·è¡Œæ›´æ–°
    report = manager.run_daily_update(
        cleanup_days=args.cleanup_days,
        batch_size=args.batch_size
    )
    
    # è¼¸å‡ºçµæœ
    exit_code = 0 if report['success'] else 1
    
    if args.test:
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼å®Œæˆ")
    elif report['success']:
        print("âœ… æ¯æ—¥æ›´æ–°æˆåŠŸå®Œæˆ")
    else:
        print("âŒ æ¯æ—¥æ›´æ–°å¤±æ•—")
        
    sys.exit(exit_code)

if __name__ == '__main__':
    main() 