import time
import json
import os
import re
from urllib.parse import parse_qs, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException, InvalidSessionIdException
import random
import traceback
import tempfile
import shutil

LOG_FOLDER = "logs"
if not os.path.exists(LOG_FOLDER):
    os.makedirs(LOG_FOLDER)

# ç¢ºä¿dataç›®éŒ„å­˜åœ¨
DATA_FOLDER = "data"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

def setup_browser():
    """è¨­ç½®Chromeç€è¦½å™¨ï¼Œå¢å¼·åçˆ¬èŸ²èƒ½åŠ›"""
    from selenium.webdriver.chrome.options import Options
    
    # ç¾ä»£ç€è¦½å™¨çš„ä½¿ç”¨è€…ä»£ç†ï¼Œå®šæœŸæ›´æ–°
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
    ]
    user_agent = random.choice(user_agents)
    
    # 1. è¨­ç½®ç€è¦½å™¨é…ç½®
    try:
        print("å‰µå»ºChromeç€è¦½å™¨...")
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--incognito")
        chrome_options.add_argument(f"--user-agent={user_agent}")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        browser = webdriver.Chrome(options=chrome_options)
        
        # è¨­ç½®æ›´é•·çš„è¶…æ™‚ç­‰å¾…
        browser.set_page_load_timeout(120)  # å¢åŠ åˆ°120ç§’
        browser.implicitly_wait(10)
        
        # éš±è—webdriverç‰¹å¾µ
        browser.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return browser
    except Exception as e:
        print(f"ç€è¦½å™¨å‰µå»ºå¤±æ•—: {e}")
        
        # 2. å‚™ç”¨é…ç½®
        try:
            print("å˜—è©¦ä½¿ç”¨å‚™ç”¨é…ç½®...")
            chrome_options = Options()
            chrome_options.add_argument("--headless=new")
            chrome_options.add_argument("--no-sandbox")
            
            browser = webdriver.Chrome(options=chrome_options)
            browser.set_page_load_timeout(60)
            print("å‚™ç”¨é…ç½®æˆåŠŸ!")
            return browser
        except Exception as e:
            print(f"å‚™ç”¨é…ç½®å¤±æ•—: {e}")
            raise e  # å¦‚æœé€£å‚™ç”¨é…ç½®éƒ½å¤±æ•—ï¼Œå‰‡å‘ä¸Šå‚³éç•°å¸¸

def safe_get_page(browser, url, max_retries=3):
    """å®‰å…¨åœ°è¨ªå•é é¢ï¼Œå¸¶é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            print(f"å˜—è©¦è¨ªå•é é¢ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡): {url}")
            browser.get(url)
            
            # ç­‰å¾…é é¢è¼‰å…¥
            time.sleep(random.uniform(3, 6))
            
            # æª¢æŸ¥é é¢æ˜¯å¦è¼‰å…¥æˆåŠŸ
            if "ç§Ÿå±‹" in browser.title or "house" in browser.current_url:
                print("âœ… é é¢è¼‰å…¥æˆåŠŸ")
                return True
            else:
                print(f"âš ï¸  é é¢è¼‰å…¥ç•°å¸¸ï¼Œæ¨™é¡Œ: {browser.title}")
                
        except Exception as e:
            print(f"âŒ è¨ªå•é é¢å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
            if attempt < max_retries - 1:
                wait_time = random.uniform(5, 10)
                print(f"ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
            else:
                print("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
                return False
    
    return False

def detect_region_filters(browser):
    """æª¢æ¸¬ç¶²ç«™æ˜¯å¦æœ‰åœ°å€ç¯©é¸åŠŸèƒ½"""
    try:
        print("ğŸ” æª¢æ¸¬åœ°å€ç¯©é¸åŠŸèƒ½...")
        
        # æª¢æŸ¥å¸¸è¦‹çš„ç¯©é¸å…ƒç´ 
        filter_selectors = [
            "select[name*='city']",
            "select[name*='region']", 
            "select[name*='area']",
            ".filter-city",
            ".region-select",
            "[data-test*='city']",
            "[data-test*='region']",
            "button",  # æª¢æŸ¥æ‰€æœ‰æŒ‰éˆ•ï¼Œç¨å¾Œç¯©é¸åŒ…å«'æ–°åŒ—'çš„
            "a",       # æª¢æŸ¥æ‰€æœ‰é€£çµï¼Œç¨å¾Œç¯©é¸åŒ…å«'æ–°åŒ—'çš„
            "select",  # æª¢æŸ¥æ‰€æœ‰ä¸‹æ‹‰é¸å–®
            ".dropdown"
        ]
        
        found_filters = []
        for selector in filter_selectors:
            try:
                elements = browser.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    for element in elements:
                        # æª¢æŸ¥å…ƒç´ çš„æ–‡å­—å…§å®¹
                        text_content = element.text
                        html_content = element.get_attribute('outerHTML')[:200]
                        
                        # å¦‚æœæ˜¯æŒ‰éˆ•æˆ–é€£çµï¼Œæª¢æŸ¥æ˜¯å¦åŒ…å«åœ°å€ç›¸é—œæ–‡å­—
                        if selector in ["button", "a"]:
                            if any(keyword in text_content for keyword in ['æ–°åŒ—', 'å°åŒ—', 'æ¡ƒåœ’', 'åœ°å€', 'åŸå¸‚']):
                                found_filters.append(f"åœ°å€ç›¸é—œ {selector}: {text_content[:50]} | HTML: {html_content[:100]}")
                        else:
                            found_filters.append(f"ç¯©é¸å…ƒç´  {selector}: {text_content[:50]} | HTML: {html_content[:100]}")
            except Exception as e:
                print(f"æª¢æŸ¥é¸æ“‡å™¨ {selector} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        if found_filters:
            print("âœ… æ‰¾åˆ°å¯èƒ½çš„ç¯©é¸å…ƒç´ :")
            for filter_info in found_filters:
                print(f"   - {filter_info}")
        else:
            print("âŒ æœªæ‰¾åˆ°æ˜é¡¯çš„åœ°å€ç¯©é¸åŠŸèƒ½")
            
        # æª¢æŸ¥é é¢URLå’Œå…§å®¹ï¼Œçœ‹æ˜¯å¦æœ‰åœ°å€ç›¸é—œçš„è³‡è¨Š
        current_url = browser.current_url
        page_source = browser.page_source
        
        print(f"ç•¶å‰URL: {current_url}")
        if "æ–°åŒ—" in page_source:
            print("âœ… é é¢å…§å®¹åŒ…å«'æ–°åŒ—'å­—æ¨£")
        else:
            print("âŒ é é¢å…§å®¹ä¸åŒ…å«'æ–°åŒ—'å­—æ¨£")
            
        return found_filters
        
    except Exception as e:
        print(f"æª¢æ¸¬ç¯©é¸åŠŸèƒ½æ™‚å‡ºéŒ¯: {e}")
        return []

def determine_city_from_coordinates(lat, lng):
    """æ ¹æ“šç¶“ç·¯åº¦åˆ¤æ–·åŸå¸‚"""
    if lat is None or lng is None:
        return "æœªçŸ¥"
    
    # å°åŒ—å¸‚é‚Šç•Œ (ç²¾ç¢ºçš„æ ¸å¿ƒå¸‚å€ï¼Œé¿å…èˆ‡æ–°åŒ—å¸‚é‡ç–Š)
    taipei_boundaries = {
        "north": 25.25,    # ç¸®å°ç¯„åœ
        "south": 24.97,    
        "east": 121.61,    # ç¸®å°ç¯„åœ
        "west": 121.47     # ç¸®å°ç¯„åœ
    }
    
    # æ–°åŒ—å¸‚é‚Šç•Œ (åŒ…å«å‘¨é‚Šå€åŸŸ)
    new_taipei_boundaries = {
        "north": 25.40,   # åŒ…å«æ·¡æ°´ã€çŸ³é–€
        "south": 24.60,   # åŒ…å«çƒä¾†ã€åªæ—
        "east": 122.00,   # åŒ…å«è²¢å¯®ã€ç‘èŠ³
        "west": 121.20    # åŒ…å«æ—å£ã€äº”è‚¡ã€ä¸‰é‡
    }
    
    # æ¡ƒåœ’å¸‚é‚Šç•Œ
    taoyuan_boundaries = {
        "north": 25.300,
        "south": 24.886,
        "east": 121.466,
        "west": 121.156
    }
    
    # æ–°åŒ—å¸‚ç‰¹å®šå€åŸŸçš„ç²¾ç¢ºåˆ¤æ–·ï¼ˆå„ªå…ˆï¼‰
    new_taipei_specific_areas = [
        # ä¸‰é‡å€
        {"north": 25.08, "south": 25.04, "east": 121.50, "west": 121.48},
        # è˜†æ´²å€  
        {"north": 25.10, "south": 25.07, "east": 121.49, "west": 121.46},
        # äº”è‚¡å€
        {"north": 25.12, "south": 25.08, "east": 121.47, "west": 121.42},
    ]
    
    # å…ˆæª¢æŸ¥æ–°åŒ—å¸‚ç‰¹å®šå€åŸŸ
    for area in new_taipei_specific_areas:
        if (area["south"] <= lat <= area["north"] and 
            area["west"] <= lng <= area["east"]):
            return "æ–°åŒ—å¸‚"
    
    # å†æª¢æŸ¥å°åŒ—å¸‚æ ¸å¿ƒå€åŸŸ
    if (taipei_boundaries["south"] <= lat <= taipei_boundaries["north"] and 
        taipei_boundaries["west"] <= lng <= taipei_boundaries["east"]):
        return "å°åŒ—å¸‚"
    
    # æœ€å¾Œæª¢æŸ¥æ–°åŒ—å¸‚å¤§ç¯„åœ
    elif (new_taipei_boundaries["south"] <= lat <= new_taipei_boundaries["north"] and 
          new_taipei_boundaries["west"] <= lng <= new_taipei_boundaries["east"]):
        return "æ–°åŒ—å¸‚"
    elif (taoyuan_boundaries["south"] <= lat <= taoyuan_boundaries["north"] and 
          taoyuan_boundaries["west"] <= lng <= taoyuan_boundaries["east"]):
        return "æ¡ƒåœ’å¸‚"
    else:
        return "å…¶ä»–"

def extract_coordinates(url):
    """å¾Google Maps URLä¸­æå–ç¶“ç·¯åº¦"""
    try:
        # è§£æURL
        parsed_url = urlparse(url)
        
        # å¾queryåƒæ•¸ä¸­ç²å–ç¶“ç·¯åº¦
        query_params = parse_qs(parsed_url.query)
        if 'query' in query_params:
            # queryæ ¼å¼é€šå¸¸æ˜¯ "latitude,longitude"
            coords = query_params['query'][0].split(',')
            if len(coords) == 2:
                latitude = float(coords[0])
                longitude = float(coords[1])
                return {"latitude": latitude, "longitude": longitude}
        
        # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±æ•—ï¼Œå˜—è©¦å¾URLä¸­ç›´æ¥æå–æ•¸å­—
        coords_match = re.search(r'query=(-?\d+\.\d+),(-?\d+\.\d+)', url)
        if coords_match:
            latitude = float(coords_match.group(1))
            longitude = float(coords_match.group(2))
            return {"latitude": latitude, "longitude": longitude}
            
        print(f"ç„¡æ³•å¾URLä¸­æå–ç¶“ç·¯åº¦: {url}")
        return {"latitude": None, "longitude": None}
    except Exception as e:
        print(f"æå–ç¶“ç·¯åº¦æ™‚å‡ºéŒ¯: {e}")
        return {"latitude": None, "longitude": None}

def crawl_house_details(browser, house_url, target_region=None):
    """çˆ¬å–å–®å€‹æˆ¿å±‹çš„è©³ç´°è³‡è¨Š"""
    try:
        # è¨ªå•æˆ¿å±‹è©³æƒ…é 
        print(f"è¨ªå•æˆ¿å±‹è©³æƒ…é : {house_url}")
        if not safe_get_page(browser, house_url):
            print(f"âŒ ç„¡æ³•è¨ªå•æˆ¿å±‹é é¢: {house_url}")
            return None
        
        # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
        time.sleep(2)
        
        house_data = {"url": house_url}
        
        # çˆ¬å–æˆ¿å±‹åç¨±
        try:
            title_element = browser.find_element(By.CSS_SELECTOR, "h1.mb-3.text-2xl.font-bold")
            house_data["title"] = title_element.text.strip()
            print(f"æˆ¿å±‹åç¨±: {house_data['title']}")
        except Exception as e:
            print(f"âš ï¸  æŠ“å–æˆ¿å±‹åç¨±æ™‚å‡ºéŒ¯: {e}")
            # å˜—è©¦å…¶ä»–é¸æ“‡å™¨
            try:
                title_element = browser.find_element(By.CSS_SELECTOR, "h1")
                house_data["title"] = title_element.text.strip()
                print(f"æˆ¿å±‹åç¨± (å‚™ç”¨é¸æ“‡å™¨): {house_data['title']}")
            except:
                print(f"âŒ ç„¡æ³•ç²å–æˆ¿å±‹åç¨±ï¼Œè·³éæ­¤æˆ¿å±‹")
                return None
        
        # çˆ¬å–åƒ¹æ ¼ - ç›´æ¥å¾é é¢æºç¢¼æœå°‹ï¼Œæœ€å¯é çš„æ–¹æ³•
        price = "0"
        
        try:
            page_source = browser.page_source
            import re
            
            # å°‹æ‰¾æ‰€æœ‰åƒ¹æ ¼æ¨¡å¼ï¼ŒæŒ‰å„ªå…ˆç´šæ’åº
            price_patterns = [
                r'(\d{1,3}(?:,\d{3})*)\s*å…ƒ/æœˆ',  # æ¨™æº–æ ¼å¼ï¼š12,000å…ƒ/æœˆ
                r'(\d{1,3}(?:,\d{3})*)\s*å…ƒ',     # ç°¡å–®æ ¼å¼ï¼š12,000å…ƒ
                r'ç§Ÿé‡‘[ï¼š:]\s*(\d{1,3}(?:,\d{3})*)', # ç§Ÿé‡‘ï¼š12000
                r'æœˆç§Ÿ[ï¼š:]\s*(\d{1,3}(?:,\d{3})*)', # æœˆç§Ÿï¼š12000
            ]
            
            for pattern in price_patterns:
                matches = re.findall(pattern, page_source)
                if matches:
                    # æ‰¾åˆ°æ•¸å­—ï¼Œå–æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯æ­£ç¢ºçš„ç§Ÿé‡‘ï¼‰
                    prices = [int(match.replace(',', '')) for match in matches]
                    max_price = max(prices)
                    if max_price >= 5000:  # åˆç†çš„ç§Ÿé‡‘ç¯„åœ
                        price = str(max_price)
                        print(f"åƒ¹æ ¼: {price} (å¾æ¨¡å¼ '{pattern}' æ‰¾åˆ°)")
                        break
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œæœå°‹æ›´å¯¬æ³›çš„æ•¸å­—æ¨¡å¼
            if price == "0":
                all_numbers = re.findall(r'(\d{1,3}(?:,\d{3})*)', page_source)
                candidate_prices = []
                for num_str in all_numbers:
                    num = int(num_str.replace(',', ''))
                    # åˆç†çš„ç§Ÿé‡‘ç¯„åœï¼š5000-200000
                    if 5000 <= num <= 200000:
                        candidate_prices.append(num)
                
                if candidate_prices:
                    # å–å‡ºç¾é »ç‡æœ€é«˜çš„åƒ¹æ ¼ï¼Œæˆ–è€…æœ€å¤§çš„åƒ¹æ ¼
                    from collections import Counter
                    counter = Counter(candidate_prices)
                    if counter:
                        price = str(counter.most_common(1)[0][0])
                        print(f"åƒ¹æ ¼: {price} (å€™é¸åƒ¹æ ¼ä¸­æœ€å¸¸è¦‹çš„)")
            
            if price == "0":
                print(f"âš ï¸  ç„¡æ³•ç²å–æœ‰æ•ˆåƒ¹æ ¼ï¼Œä½¿ç”¨é è¨­å€¼ 0")
                price = "0"
                
        except Exception as e:
            print(f"âš ï¸  åƒ¹æ ¼æŠ“å–ç•°å¸¸: {e}ï¼Œä½¿ç”¨é è¨­å€¼ 0")
            price = "0"
        
        house_data["price"] = price
        
        # çˆ¬å–åŸºæœ¬è³‡æ–™ï¼ˆåªæ•¸ã€åœ°å€ã€æ ¼å±€ç­‰ï¼‰
        try:
            print("ğŸ“‹ é–‹å§‹æŠ“å–åŸºæœ¬è³‡æ–™...")
            
            # æŠ“å–åœ°å€ - å˜—è©¦å¤šç¨®é¸æ“‡å™¨
            address = None
            try:
                # æ–¹æ³•1: å°‹æ‰¾åœ°å€ç›¸é—œçš„å…ƒç´ 
                address_elements = browser.find_elements(By.XPATH, "//span[contains(text(), 'åœ°å€')]/following-sibling::span")
                if address_elements:
                    address = address_elements[0].text.strip()
                    print(f"   åœ°å€ (æ–¹æ³•1): {address}")
                
                # æ–¹æ³•2: å¾é é¢sourceä¸­æœå°‹åœ°å€æ¨¡å¼
                if not address:
                    page_source = browser.page_source
                    import re
                    # åŒ¹é…å°ç£åœ°å€æ¨¡å¼
                    address_patterns = [
                        r'(å°åŒ—å¸‚|æ–°åŒ—å¸‚|åŸºéš†å¸‚|æ¡ƒåœ’å¸‚|æ–°ç«¹å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å°ä¸­å¸‚|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©å¸‚|å˜‰ç¾©ç¸£|å°å—å¸‚|é«˜é›„å¸‚|å±æ±ç¸£|å°æ±ç¸£|èŠ±è“®ç¸£|å®œè˜­ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)[^<>"]{10,50}',
                    ]
                    
                    for pattern in address_patterns:
                        address_match = re.search(pattern, page_source)
                        if address_match:
                            address = address_match.group(0)
                            print(f"   åœ°å€ (æ­£å‰‡è¡¨é”å¼): {address}")
                            break
                
                if address:
                    house_data["address"] = address
                else:
                    house_data["address"] = house_data["title"]  # å‚™ç”¨
                    print("   âš ï¸  ç„¡æ³•ç²å–åœ°å€ï¼Œä½¿ç”¨æ¨™é¡Œä½œç‚ºåœ°å€")
                    
            except Exception as addr_e:
                print(f"   âš ï¸  æŠ“å–åœ°å€å¤±æ•—: {addr_e}")
                house_data["address"] = house_data["title"]
            
            # æŠ“å–åŸºæœ¬è³‡æ–™ - æ ¹æ“šæ–°çš„HTMLçµæ§‹å„ªåŒ–
            try:
                # æ–¹æ³•1: ç²¾ç¢ºæŠ“å– base_info å€å¡Š
                base_info_section = browser.find_element(By.CSS_SELECTOR, "div.base_info")
                
                # æŠ“å–åªæ•¸
                ping_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'åªæ•¸')]/following-sibling::span")
                ping_text = ping_element.text.strip()
                
                if ping_text and "åª" in ping_text:
                    print(f"   åªæ•¸: {ping_text}")
                    # æå–æ•¸å­—éƒ¨åˆ†ï¼šæ”¯æ´"ä½¿ç”¨8åª"ã€"8åª"ã€"8.5åª"ç­‰æ ¼å¼  
                    import re
                    numbers = re.findall(r'([0-9]+\.?[0-9]*)', ping_text)
                    if numbers:
                        house_data["size"] = numbers[0]
                        house_data["size_detail"] = ping_text  # ä¿å­˜å®Œæ•´æè¿°
                        print(f"   è§£æåªæ•¸: {house_data['size']} åª")
                    else:
                        house_data["size"] = "0"
                        house_data["size_detail"] = ping_text
                else:
                    raise Exception("åœ¨base_infoä¸­æ‰¾ä¸åˆ°åªæ•¸")
                    
            except Exception as base_info_e:
                print(f"   âš ï¸  base_infoæŠ“å–å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•: {base_info_e}")
                try:
                    # æ–¹æ³•2: å‚™ç”¨é¸æ“‡å™¨
                    ping_elements = browser.find_elements(By.XPATH, "//span[contains(text(), 'åªæ•¸')]/following-sibling::span | //div[contains(text(), 'åªæ•¸')]/following-sibling::div")
                    
                    ping_text = None
                    for element in ping_elements:
                        text = element.text.strip()
                        if "åª" in text and any(char.isdigit() for char in text):
                            ping_text = text
                            break
                    
                    if ping_text:
                        print(f"   åªæ•¸ (å‚™ç”¨æ–¹æ³•): {ping_text}")
                        numbers = re.findall(r'([0-9]+\.?[0-9]*)', ping_text)
                        if numbers:
                            house_data["size"] = numbers[0]
                            house_data["size_detail"] = ping_text
                            print(f"   è§£æåªæ•¸: {house_data['size']} åª")
                        else:
                            house_data["size"] = "0"
                            house_data["size_detail"] = ping_text
                    else:
                        # æ–¹æ³•3: å¾é é¢sourceæœå°‹åªæ•¸
                        page_source = browser.page_source
                        ping_match = re.search(r'ä½¿ç”¨([0-9]+\.?[0-9]*)\s*åª|æ¬Šç‹€([0-9]+\.?[0-9]*)\s*åª|([0-9]+\.?[0-9]*)\s*åª', page_source)
                        if ping_match:
                            ping_num = ping_match.group(1) or ping_match.group(2) or ping_match.group(3)
                            house_data["size"] = ping_num
                            house_data["size_detail"] = ping_match.group(0)
                            print(f"   åªæ•¸ (æ­£å‰‡): {house_data['size']} åª")
                        else:
                            house_data["size"] = "0"
                            house_data["size_detail"] = "æœªæä¾›"
                            print("   âš ï¸  ç„¡æ³•ç²å–åªæ•¸")
                            
                except Exception as size_e:
                    print(f"   âš ï¸  æŠ“å–åªæ•¸å¤±æ•—: {size_e}")
                    house_data["size"] = "0"
                    house_data["size_detail"] = "æœªæä¾›"
            
            # ç¹¼çºŒåœ¨åŒä¸€å€‹ base_info å€å¡Šä¸­æŠ“å–å…¶ä»–è³‡æ–™
            try:
                # åœ¨å·²æ‰¾åˆ°çš„ base_info_section ä¸­æŠ“å–æ‰€æœ‰è³‡æ–™
                
                # æŠ“å–æ ¼å±€è³‡è¨Š
                try:
                    layout_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'æ ¼å±€')]/following-sibling::span")
                    layout = layout_element.text.strip()
                    house_data["room_layout"] = layout
                    print(f"   æ ¼å±€: {layout}")
                except:
                    # å‚™ç”¨æ–¹æ³•
                    layout_elements = browser.find_elements(By.XPATH, "//span[contains(text(), 'æ ¼å±€')]/following-sibling::span")
                    layout = None
                    for element in layout_elements:
                        text = element.text.strip()
                        if "æˆ¿" in text or "å»³" in text or "è¡›" in text:
                            layout = text
                            break
                    
                    if not layout:
                        # å¾é é¢æœå°‹æ ¼å±€æ¨¡å¼
                        page_source = browser.page_source
                        layout_match = re.search(r'([0-9]+æˆ¿[0-9]*å»³[0-9]*è¡›[^<>"]*)', page_source)
                        if layout_match:
                            layout = layout_match.group(1)
                    
                    house_data["room_layout"] = layout or "æœªæä¾›"
                    print(f"   æ ¼å±€ (å‚™ç”¨): {house_data['room_layout']}")
                
                # æŠ“å–æ¨“å±¤è³‡è¨Š
                try:
                    floor_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'æ¨“å±¤')]/following-sibling::span")
                    floor = floor_element.text.strip()
                    house_data["floor_info"] = floor
                    print(f"   æ¨“å±¤: {floor}")
                except:
                    # å‚™ç”¨æ–¹æ³•
                    floor_elements = browser.find_elements(By.XPATH, "//span[contains(text(), 'æ¨“å±¤')]/following-sibling::span")
                    floor = None
                    for element in floor_elements:
                        text = element.text.strip()
                        if "æ¨“" in text and any(char.isdigit() for char in text):
                            floor = text
                            break
                    
                    house_data["floor_info"] = floor or "æœªæä¾›"
                    print(f"   æ¨“å±¤ (å‚™ç”¨): {house_data['floor_info']}")
                
                # æŠ“å–æˆ¿å±‹ç¾æ³å’Œå‹æ…‹
                try:
                    # ç¾æ³ (å¦‚ï¼šåˆ†ç§Ÿå¥—æˆ¿)
                    status_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'ç¾æ³')]/following-sibling::span")
                    house_status = status_element.text.strip()
                    
                    # å‹æ…‹ (å¦‚ï¼šå…¬å¯“)
                    type_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'å‹æ…‹')]/following-sibling::span")
                    house_type = type_element.text.strip()
                    
                    # åˆä½µç¾æ³å’Œå‹æ…‹
                    house_data["house_type"] = f"{house_status} ({house_type})"
                    print(f"   æˆ¿å±‹é¡å‹: {house_data['house_type']}")
                    
                except:
                    # å‚™ç”¨æ–¹æ³• - åˆ†åˆ¥å˜—è©¦
                    try:
                        type_elements = browser.find_elements(By.XPATH, "//span[contains(text(), 'å‹æ…‹')]/following-sibling::span | //span[contains(text(), 'ç¾æ³')]/following-sibling::span")
                        house_type = None
                        for element in type_elements:
                            text = element.text.strip()
                            if text and text not in ["", "-"]:
                                house_type = text
                                break
                        
                        house_data["house_type"] = house_type or "æœªæä¾›"
                        print(f"   æˆ¿å±‹é¡å‹ (å‚™ç”¨): {house_data['house_type']}")
                    except:
                        house_data["house_type"] = "æœªæä¾›"
                        print("   âš ï¸  ç„¡æ³•ç²å–æˆ¿å±‹é¡å‹")
                
                # æŠ“å–è»Šä½è³‡è¨Š (æ–°å¢)
                try:
                    parking_element = base_info_section.find_element(By.XPATH, ".//span[contains(text(), 'è»Šä½')]/following-sibling::span")
                    parking = parking_element.text.strip()
                    house_data["parking"] = parking
                    print(f"   è»Šä½: {parking}")
                except:
                    house_data["parking"] = "æœªæä¾›"
                    print("   è»Šä½: æœªæä¾›")
                
            except Exception as base_info_extract_e:
                print(f"   âš ï¸  æŠ“å–base_infoå…¶ä»–è³‡æ–™å¤±æ•—: {base_info_extract_e}")
                # è¨­å®šé è¨­å€¼
                house_data["room_layout"] = house_data.get("room_layout", "æœªæä¾›")
                house_data["floor_info"] = house_data.get("floor_info", "æœªæä¾›") 
                house_data["house_type"] = house_data.get("house_type", "æœªæä¾›")
                house_data["parking"] = house_data.get("parking", "æœªæä¾›")
                
        except Exception as e:
            print(f"âš ï¸  æŠ“å–åŸºæœ¬è³‡æ–™æ™‚å‡ºéŒ¯: {e}")
            # è¨­å®šé è¨­å€¼
            house_data["address"] = house_data["title"]
            house_data["size"] = "0"
            house_data["size_detail"] = "æœªæä¾›"
            house_data["room_layout"] = "æœªæä¾›"
            house_data["floor_info"] = "æœªæä¾›"
            house_data["house_type"] = "æœªæä¾›"
            house_data["parking"] = "æœªæä¾›"
        
        # çˆ¬å–æˆ¿å±‹åœ–ç‰‡ï¼ˆæ–°å¢ï¼‰
        try:
            print("ğŸ–¼ï¸  é–‹å§‹æŠ“å–æˆ¿å±‹åœ–ç‰‡...")
            images = []
            
            # æ–¹æ³•1: å°‹æ‰¾åœ–ç‰‡å…‰ç®±å®¹å™¨ï¼ˆoverflow-autoï¼‰
            try:
                lightbox_container = browser.find_element(By.CSS_SELECTOR, "div.overflow-auto")
                image_elements = lightbox_container.find_elements(By.CSS_SELECTOR, "img")
                
                for img in image_elements[:2]:  # åªå–å‰2å¼µåœ–ç‰‡
                    img_src = img.get_attribute("src")
                    img_data_src = img.get_attribute("data-src")
                    
                    # å„ªå…ˆä½¿ç”¨data-srcï¼Œæ²’æœ‰å‰‡ä½¿ç”¨src
                    image_url = img_data_src if img_data_src else img_src
                    
                    if image_url and image_url.startswith("http"):
                        images.append(image_url)
                        print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡: {image_url}")
                
                print(f"å¾å…‰ç®±å®¹å™¨æŠ“å–åˆ° {len(images)} å¼µåœ–ç‰‡")
                
            except Exception as lightbox_e:
                print(f"âš ï¸  å…‰ç®±æ–¹æ³•å¤±æ•—: {lightbox_e}")
            
            # æ–¹æ³•2: å¦‚æœå…‰ç®±æ–¹æ³•å¤±æ•—ï¼Œå˜—è©¦é€šç”¨åœ–ç‰‡é¸æ“‡å™¨
            if len(images) < 2:
                print("å˜—è©¦é€šç”¨åœ–ç‰‡é¸æ“‡å™¨...")
                try:
                    all_images = browser.find_elements(By.CSS_SELECTOR, "img[src*='houseprice.tw'], img[data-src*='houseprice.tw']")
                    
                    for img in all_images:
                        if len(images) >= 2:
                            break
                            
                        img_src = img.get_attribute("src")
                        img_data_src = img.get_attribute("data-src")
                        
                        image_url = img_data_src if img_data_src else img_src
                        
                        if (image_url and 
                            image_url.startswith("http") and 
                            "houseprice.tw" in image_url and
                            image_url not in images):
                            images.append(image_url)
                            print(f"   âœ… æ‰¾åˆ°åœ–ç‰‡: {image_url}")
                    
                    print(f"é€šç”¨æ–¹æ³•é¡å¤–æŠ“å–åˆ° {len(images)} å¼µåœ–ç‰‡")
                    
                except Exception as general_e:
                    print(f"âš ï¸  é€šç”¨æ–¹æ³•å¤±æ•—: {general_e}")
            
            # ä¿å­˜åœ–ç‰‡åˆ°è³‡æ–™ä¸­
            house_data["images"] = images[:2]  # æœ€å¤šä¿å­˜2å¼µåœ–ç‰‡
            print(f"âœ… ç¸½å…±ä¿å­˜ {len(house_data['images'])} å¼µåœ–ç‰‡")
            
        except Exception as e:
            print(f"âš ï¸  æŠ“å–åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")
            house_data["images"] = []  # ç©ºé™£åˆ—ï¼Œä¸å½±éŸ¿å…¶ä»–è³‡æ–™
        
        # çˆ¬å–åœ°ç†åº§æ¨™ - æ”¹é€²ç‰ˆæœ¬
        try:
            page_source = browser.page_source
            import re
            
            # å¤šç¨®åº§æ¨™æ¨¡å¼æœå°‹
            coordinate_patterns = [
                r'"lat"[:\s]*([0-9.-]+)',  # "lat": 25.xxx
                r'"latitude"[:\s]*([0-9.-]+)',  # "latitude": 25.xxx
                r'lat[:\s]*([0-9.-]+)',   # lat: 25.xxx
                r'latitude[:\s]*([0-9.-]+)',  # latitude: 25.xxx
                r'lat=([0-9.-]+)',        # lat=25.xxx
            ]
            
            longitude_patterns = [
                r'"lng"[:\s]*([0-9.-]+)',  # "lng": 121.xxx
                r'"longitude"[:\s]*([0-9.-]+)',  # "longitude": 121.xxx
                r'lng[:\s]*([0-9.-]+)',   # lng: 121.xxx
                r'longitude[:\s]*([0-9.-]+)',  # longitude: 121.xxx
                r'lng=([0-9.-]+)',        # lng=121.xxx
            ]
            
            lat = None
            lng = None
            
            # å˜—è©¦æ‰€æœ‰æ¨¡å¼æ‰¾åº§æ¨™
            for pattern in coordinate_patterns:
                lat_match = re.search(pattern, page_source, re.IGNORECASE)
                if lat_match:
                    potential_lat = float(lat_match.group(1))
                    if 20 <= potential_lat <= 30:  # å°ç£çš„ç·¯åº¦ç¯„åœ
                        lat = potential_lat
                        break
            
            for pattern in longitude_patterns:
                lng_match = re.search(pattern, page_source, re.IGNORECASE)
                if lng_match:
                    potential_lng = float(lng_match.group(1))
                    if 115 <= potential_lng <= 125:  # å°ç£çš„ç¶“åº¦ç¯„åœ
                        lng = potential_lng
                        break
            
            # å¦‚æœJavaScriptæ–¹æ³•å¤±æ•—ï¼Œå˜—è©¦å¾Google Mapsé€£çµæå–
            if lat is None or lng is None:
                print("å˜—è©¦å¾Google Mapsé€£çµæå–åº§æ¨™...")
                try:
                    map_elements = browser.find_elements(By.CSS_SELECTOR, "a[href*='google.com/maps']")
                    if not map_elements:
                        map_elements = browser.find_elements(By.CSS_SELECTOR, "a[href*='maps.google']")
                    
                    for map_element in map_elements:
                        map_url = map_element.get_attribute("href")
                        if map_url:
                            coords = extract_coordinates(map_url)
                            if coords["latitude"] and coords["longitude"]:
                                lat = coords["latitude"]
                                lng = coords["longitude"]
                                print("âœ… å¾Google Mapsé€£çµæˆåŠŸæå–åº§æ¨™")
                                break
                except Exception as map_e:
                    print(f"Google Mapsé€£çµæå–å¤±æ•—: {map_e}")
            
            if lat is not None and lng is not None:
                house_data["latitude"] = lat
                house_data["longitude"] = lng
                
                # ç›´æ¥ä½¿ç”¨ç›®æ¨™åœ°å€ï¼Œä¸ç”¨ç¶“ç·¯åº¦åˆ¤æ–·
                house_data["detected_city"] = target_region or "æœªçŸ¥"
                print(f"åœ°ç†åº§æ¨™: {lat}, {lng} ({target_region})")
            else:
                print("âš ï¸  ç„¡æ³•ç²å–åœ°ç†åº§æ¨™ï¼Œä½¿ç”¨é è¨­å€¼")
                house_data["detected_city"] = target_region or "æœªçŸ¥"
                house_data["latitude"] = None
                house_data["longitude"] = None
                # ä¸è¦å› ç‚ºåº§æ¨™å•é¡Œå°±è·³éæˆ¿å±‹ï¼Œä»ç„¶è¿”å›å…¶ä»–è³‡è¨Š
                
        except Exception as e:
            print(f"âš ï¸  æŠ“å–åœ°ç†åº§æ¨™æ™‚å‡ºéŒ¯: {e}")
            house_data["detected_city"] = target_region or "æœªçŸ¥"
            house_data["latitude"] = None
            house_data["longitude"] = None

        # æŠ“å–åœ°å€ä¿¡æ¯ï¼ˆå¾å°èˆªbreadcrumbï¼‰
        try:
            print("ğŸ—ºï¸  é–‹å§‹æŠ“å–åœ°å€ä¿¡æ¯...")
            district = None
            city = None
            
            # æ–¹æ³•1: å¾å°èˆªbreadcrumbæŠ“å–
            try:
                # å°‹æ‰¾å°èˆªå®¹å™¨
                nav_selectors = [
                    "nav.flex.space-x-2",  # å…·é«”çš„å°èˆªæ¨£å¼
                    "nav[class*='breadcrumb']",
                    "div[class*='breadcrumb']",
                    ".breadcrumb",
                    "nav",
                    "[class*='nav']"
                ]
                
                breadcrumb_found = False
                for selector in nav_selectors:
                    try:
                        nav_elements = browser.find_elements(By.CSS_SELECTOR, selector)
                        for nav_element in nav_elements:
                            nav_text = nav_element.text
                            nav_html = nav_element.get_attribute('innerHTML')
                            
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«åœ°å€ç›¸é—œçš„æ–‡å­—
                            if any(keyword in nav_text for keyword in ['å€', 'å¸‚', 'ç¸£', 'é„‰', 'é®']):
                                print(f"   æ‰¾åˆ°å°èˆªå…ƒç´ : {nav_text[:100]}")
                                
                                # å¾å°èˆªä¸­æå–åœ°å€ä¿¡æ¯
                                # å°‹æ‰¾åŒ…å«"å€"çš„é€£çµæˆ–æ–‡å­—
                                links = nav_element.find_elements(By.TAG_NAME, "a")
                                for link in links:
                                    link_text = link.text.strip()
                                    if link_text.endswith('å€') or link_text.endswith('å¸‚') or link_text.endswith('ç¸£'):
                                        if link_text.endswith('å€'):
                                            district = link_text
                                            print(f"   âœ… å¾å°èˆªæŠ“å–åˆ°åœ°å€: {district}")
                                            breadcrumb_found = True
                                            break
                                        elif link_text.endswith('å¸‚') or link_text.endswith('ç¸£'):
                                            city = link_text
                                            print(f"   âœ… å¾å°èˆªæŠ“å–åˆ°åŸå¸‚: {city}")
                                
                                if breadcrumb_found:
                                    break
                    except Exception as nav_e:
                        continue
                    
                    if breadcrumb_found:
                        break
                        
            except Exception as breadcrumb_e:
                print(f"   å°èˆªæ–¹æ³•å¤±æ•—: {breadcrumb_e}")
            
            # æ–¹æ³•2: å¾é é¢å…§å®¹æœå°‹åœ°å€æ¨¡å¼
            if not district:
                print("   å˜—è©¦å¾é é¢å…§å®¹æœå°‹åœ°å€...")
                try:
                    page_source = browser.page_source
                    
                    # å°ç£å¸¸è¦‹çš„è¡Œæ”¿å€åˆ—è¡¨
                    districts = [
                        # å°åŒ—å¸‚
                        'ä¸­æ­£å€', 'å¤§åŒå€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å¤§å®‰å€', 'è¬è¯å€', 'ä¿¡ç¾©å€', 'å£«æ—å€', 'åŒ—æŠ•å€', 'å…§æ¹–å€', 'å—æ¸¯å€', 'æ–‡å±±å€',
                        # æ–°åŒ—å¸‚
                        'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'æ¨¹æ—å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'æ±æ­¢å€', 'ç‘èŠ³å€',
                        'åœŸåŸå€', 'è˜†æ´²å€', 'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'çƒä¾†å€', 'é‡‘å±±å€', 'è¬é‡Œå€', 'çŸ³é–€å€',
                        'ä¸‰èŠå€', 'è²¢å¯®å€', 'å¹³æºªå€', 'é›™æºªå€', 'å…«é‡Œå€',
                        # æ¡ƒåœ’å¸‚
                        'æ¡ƒåœ’å€', 'ä¸­å£¢å€', 'å¤§æºªå€', 'æ¥Šæ¢…å€', 'è˜†ç«¹å€', 'å¤§åœ’å€', 'é¾œå±±å€', 'å…«å¾·å€', 'é¾æ½­å€', 'å¹³é®å€', 'æ–°å±‹å€', 'è§€éŸ³å€', 'å¾©èˆˆå€'
                    ]
                    
                    # æœå°‹é é¢ä¸­æ˜¯å¦åŒ…å«é€™äº›åœ°å€åç¨±
                    for district_name in districts:
                        if district_name in page_source:
                            # é€²ä¸€æ­¥é©—è­‰é€™æ˜¯å¦çœŸçš„æ˜¯åœ°å€ä¿¡æ¯ï¼ˆä¸æ˜¯ç„¡é—œçš„æ–‡å­—ï¼‰
                            context_patterns = [
                                rf'{district_name}[^å€å¸‚ç¸£]*(?:å‡ºç§Ÿ|ç§Ÿå±‹|æˆ¿å±‹)',
                                rf'(?:ä½æ–¼|åœ¨){district_name}',
                                rf'{district_name}(?:çš„|åœ°å€)',
                                rf'href="[^"]*{district_name}[^"]*"[^>]*>{district_name}</a>'
                            ]
                            
                            for pattern in context_patterns:
                                if re.search(pattern, page_source):
                                    district = district_name
                                    print(f"   âœ… å¾é é¢å…§å®¹æ‰¾åˆ°åœ°å€: {district}")
                                    break
                            
                            if district:
                                break
                                
                except Exception as content_e:
                    print(f"   é é¢å…§å®¹æœå°‹å¤±æ•—: {content_e}")
            
            # æ–¹æ³•3: å¾åœ°å€ä¸­æå–åœ°å€
            if not district and house_data.get("address"):
                print("   å˜—è©¦å¾åœ°å€æå–åœ°å€...")
                try:
                    address = house_data["address"]
                    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å¾åœ°å€ä¸­æå–è¡Œæ”¿å€
                    district_match = re.search(r'(å°åŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|åŸºéš†å¸‚|æ–°ç«¹å¸‚|æ–°ç«¹ç¸£)([^å¸‚ç¸£]*å€)', address)
                    if district_match:
                        city = district_match.group(1)
                        district = district_match.group(2)
                        print(f"   âœ… å¾åœ°å€æå–åˆ°: {city} {district}")
                        
                except Exception as addr_e:
                    print(f"   å¾åœ°å€æå–å¤±æ•—: {addr_e}")
            
            # ä¿å­˜åœ°å€ä¿¡æ¯
            house_data["district"] = district or "æœªçŸ¥"
            house_data["city"] = target_region or house_data.get("detected_city", "æœªçŸ¥")
            
            if district:
                print(f"âœ… æˆåŠŸæŠ“å–åœ°å€ä¿¡æ¯: {house_data['city']} {house_data['district']}")
            else:
                print("âš ï¸  ç„¡æ³•ç¢ºå®šå…·é«”åœ°å€ï¼Œä½¿ç”¨é è¨­å€¼")
                
        except Exception as e:
            print(f"âš ï¸  æŠ“å–åœ°å€ä¿¡æ¯æ™‚å‡ºéŒ¯: {e}")
            house_data["district"] = "æœªçŸ¥"
            house_data["city"] = house_data.get("detected_city", "æœªçŸ¥")
        
        # çŸ­æš«ä¼‘æ¯é¿å…è«‹æ±‚éå¿«
        time.sleep(random.uniform(0.5, 1.5))
        
        return house_data
        
    except Exception as e:
        print(f"âŒ çˆ¬å–æˆ¿å±‹è©³æƒ…æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        return None

def save_data_incrementally(house_data):
    """å¢é‡ä¿å­˜æ•¸æ“šï¼Œæ¯æŠ“åˆ°ä¸€ç­†å°±ä¿å­˜"""
    json_file_path = os.path.join(DATA_FOLDER, "data.json")
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_data = []
    if os.path.exists(json_file_path):
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except:
            existing_data = []
    
    # æ·»åŠ æ–°æ•¸æ“š
    existing_data.append(house_data)
    
    # ä¿å­˜å›æ–‡ä»¶
    try:
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ æ•¸æ“šå·²ä¿å­˜ï¼Œç›®å‰ç¸½è¨ˆ: {len(existing_data)} ç­†")
        return True
    except Exception as e:
        print(f"   âŒ ä¿å­˜æ•¸æ“šå¤±æ•—: {e}")
        return False

def crawl_5168_all_regions():
    """çˆ¬å–ç§Ÿå±‹ç¶²ç«™ï¼Œæ”¶é›†æ‰€æœ‰åœ°å€çš„è³‡æ–™"""
    # åˆå§‹åŒ–æ•¸æ“šæ–‡ä»¶
    json_file_path = os.path.join(DATA_FOLDER, "data.json")
    try:
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump([], f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ åˆå§‹åŒ–æ•¸æ“šæ–‡ä»¶: {json_file_path}")
    except Exception as e:
        print(f"âš ï¸  åˆå§‹åŒ–æ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
    
    browser = setup_browser()
    try:
        # è¨ªå•åˆå§‹é é¢
        if not safe_get_page(browser, "https://rent.houseprice.tw"):
            print("âŒ ç„¡æ³•è¨ªå•ä¸»é é¢ï¼ŒçµæŸçˆ¬èŸ²")
            return
        
        # æª¢æ¸¬åœ°å€ç¯©é¸åŠŸèƒ½
        detect_region_filters(browser)
        
        # å®šç¾©ä¸åŒåœ°å€çš„URL - æ¯å€‹åœ°å€æŠ“20å€‹æˆ¿å±‹
        region_urls = {
            "æ–°åŒ—å¸‚": "https://rent.houseprice.tw/list/21_usage/27-26-15-23-33-28-32-36-37-34-35-31-29-30-38-39-40-41-14-13-16-20-19-21-22-18-17-24-25_zip/?p=1",  # æ–°åŒ—å¸‚å°ˆç”¨URL - å…ˆçˆ¬æ–°åŒ—
            "å°åŒ—å¸‚": "https://rent.houseprice.tw"  # ä¸»é ä¸»è¦æ˜¯å°åŒ—å¸‚ - å¾Œçˆ¬å°åŒ—
        }
        
        all_house_data = []  # å­˜å„²æ‰€æœ‰æˆ¿å±‹çš„è©³ç´°æ•¸æ“š
        
        # å°æ¯å€‹åœ°å€åˆ†åˆ¥çˆ¬å–
        for region_name, region_url in region_urls.items():
            print(f"\nğŸ™ï¸  é–‹å§‹çˆ¬å– {region_name} çš„ç§Ÿå±‹è³‡æ–™...")
            print(f"ğŸ”— è¨ªå•URL: {region_url}")
            
            try:
                if safe_get_page(browser, region_url):
                    # æª¢æŸ¥æ­¤é é¢æ˜¯å¦æœ‰å°æ‡‰åœ°å€çš„å…§å®¹
                    page_source = browser.page_source
                    region_count = page_source.count(region_name.replace("å¸‚", ""))
                    print(f"æ­¤é é¢åŒ…å« {region_count} å€‹'{region_name.replace('å¸‚', '')}'å­—æ¨£")
                    
                    # çˆ¬å–è©²åœ°å€çš„è³‡æ–™
                    region_data = crawl_current_page(browser, region_url, region_name)
                    if region_data:
                        all_house_data.extend(region_data)
                        print(f"âœ… å¾ {region_name} æ”¶é›†åˆ° {len(region_data)} ç­†è³‡æ–™")
                        print(f"ğŸ“Š ç›®å‰ç¸½è³‡æ–™é‡: {len(all_house_data)} ç­†")
                    else:
                        print(f"âŒ å¾ {region_name} æœªæ”¶é›†åˆ°è³‡æ–™")
                    
                    print(f"ğŸ”„ {region_name} è™•ç†å®Œæˆï¼Œæº–å‚™è™•ç†ä¸‹ä¸€å€‹åœ°å€...")
                    time.sleep(5)  # åœ°å€é–“ä¼‘æ¯æ™‚é–“å¢åŠ 
                else:
                    print(f"âŒ ç„¡æ³•è¨ªå• {region_name} é é¢")
            except Exception as e:
                print(f"âŒ çˆ¬å– {region_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                print("ğŸ”„ ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹åœ°å€...")
                continue
        
        # æŒ‰åœ°å€åˆ†é¡å’Œçµ±è¨ˆ
        if all_house_data:
            print(f"\n=== è³‡æ–™å·²åˆ†é¡å®Œæˆ ===")
            
            # çµ±è¨ˆçµæœ
            city_stats = {}
            for item in all_house_data:
                city = item.get("detected_city", "æœªçŸ¥")
                city_stats[city] = city_stats.get(city, 0) + 1
            
            print("ğŸ“Š åœ°å€çµ±è¨ˆ:")
            for city, count in sorted(city_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"   {city}: {count} ç­†")
            
            # ä¿å­˜æ‰€æœ‰æ”¶é›†çš„æ•¸æ“šåˆ°JSONæ–‡ä»¶
            json_file_path = os.path.join(DATA_FOLDER, "data.json")
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(all_house_data, f, ensure_ascii=False, indent=2)
            print(f"\n=== ç¸½è¨ˆæ”¶é›†åˆ° {len(all_house_data)} å€‹æˆ¿å±‹è³‡è¨Šï¼Œå·²ä¿å­˜åˆ° {json_file_path} ===")
            
            # æª¢æŸ¥å„åœ°å€è³‡æ–™
            taipei_data = [item for item in all_house_data if item.get("detected_city") == "å°åŒ—å¸‚"]
            new_taipei_data = [item for item in all_house_data if item.get("detected_city") == "æ–°åŒ—å¸‚"]
            
            print(f"\nğŸ¯ æ”¶é›†çµæœ:")
            print(f"   å°åŒ—å¸‚: {len(taipei_data)} ç­†")
            print(f"   æ–°åŒ—å¸‚: {len(new_taipei_data)} ç­†")
            
            if new_taipei_data:
                print(f"\nğŸ‰ æˆåŠŸæ‰¾åˆ°æ–°åŒ—å¸‚è³‡æ–™! æ¨£ä¾‹:")
                for i, item in enumerate(new_taipei_data[:3]):
                    title = item.get('title', 'æœªçŸ¥')[:40]
                    price = item.get('price', 'æœªçŸ¥')
                    print(f"   {i+1}. {title} ({price})")
            else:
                print("âŒ æœªæ‰¾åˆ°æ–°åŒ—å¸‚è³‡æ–™ï¼Œå¯èƒ½éœ€è¦èª¿æ•´åœ°ç†é‚Šç•Œ")
        else:
            print("æœªæ”¶é›†åˆ°ä»»ä½•æˆ¿å±‹è³‡è¨Š")
        
    except Exception as e:
        print(f"çˆ¬å–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
    finally:
        browser.quit()
        print("çˆ¬èŸ²çµæŸ")

def crawl_current_page(browser, base_url, target_region=None):
    """çˆ¬å–ç•¶å‰é é¢åŠå…¶åˆ†é çš„æ‰€æœ‰è³‡æ–™"""
    page_data = []
    current_page = 1
    has_next_page = True
    max_pages = 3  # æ¯å€‹åœ°å€çˆ¬å–3é ï¼Œç¢ºä¿èƒ½æŠ“åˆ°20å€‹
    min_houses_per_region = 40  # æ¯å€‹åœ°å€æŠ“40ç­†è³‡æ–™
    successful_houses = 0  # æˆåŠŸçˆ¬å–çš„æˆ¿å±‹æ•¸é‡
    failed_houses = 0     # å¤±æ•—çš„æˆ¿å±‹æ•¸é‡
    
    while has_next_page and current_page <= max_pages and len(page_data) < min_houses_per_region:
        print(f"\næ­£åœ¨çˆ¬å– {target_region or 'ç›®æ¨™åœ°å€'} ç¬¬ {current_page} é ...")
        
        # æå–ç•¶å‰é é¢çš„æˆ¿å±‹é€£çµ
        house_links = browser.find_elements(By.CSS_SELECTOR, "a.group")
        
        if len(house_links) > 0:
            print(f"ç¬¬ {current_page} é æ‰¾åˆ° {len(house_links)} å€‹æˆ¿å±‹é€£çµ")
            
            # ç«‹å³æ”¶é›†æ‰€æœ‰é€£çµURLsï¼Œé¿å…DOMå¤±æ•ˆå•é¡Œ
            house_urls = []
            for i, link in enumerate(house_links):
                try:
                    href = link.get_attribute("href")
                    if href:
                        house_urls.append(href)
                        print(f"   æ”¶é›†é€£çµ {i+1}: {href}")
                except Exception as e:
                    print(f"âš ï¸  ç²å–ç¬¬ {i+1} å€‹é€£çµæ™‚å‡ºéŒ¯: {e}")
            
            print(f"æˆåŠŸæ”¶é›†åˆ° {len(house_urls)} å€‹é€£çµ")
            
            # æ ¹æ“šéœ€æ±‚è™•ç†æˆ¿å±‹æ•¸é‡
            houses_needed = min_houses_per_region - len(page_data)
            if houses_needed > 0:
                # æ¯å€‹åœ°å€åªè™•ç†1å€‹æˆ¿å±‹ï¼Œå¿«é€Ÿæ¸¬è©¦
                houses_to_process = min(len(house_urls), houses_needed)
                limited_urls = house_urls[:houses_to_process]
                print(f"è™•ç† {len(limited_urls)} å€‹æˆ¿å±‹ï¼ˆå…± {len(house_urls)} å€‹ï¼Œç›®æ¨™: {min_houses_per_region}ï¼Œå·²æœ‰: {len(page_data)}ï¼‰")
            else:
                print(f"âœ… å·²é”åˆ°æœ€å°è³‡æ–™é‡è¦æ±‚ï¼ˆ{min_houses_per_region} ç­†ï¼‰ï¼Œè·³éå‰©é¤˜æˆ¿å±‹")
                break
            
            # é€ä¸€è¨ªå•æ¯å€‹æˆ¿å±‹è©³æƒ…é 
            for i, url in enumerate(limited_urls):
                print(f"\nè™•ç† {target_region or 'ç›®æ¨™åœ°å€'} ç¬¬ {current_page} é çš„ç¬¬ {i+1}/{len(limited_urls)} å€‹æˆ¿å±‹")
                
                try:
                    house_data = crawl_house_details(browser, url, target_region)
                    if house_data:
                        # ç«‹å³ä¿å­˜æ•¸æ“š
                        if save_data_incrementally(house_data):
                            page_data.append(house_data)
                            successful_houses += 1
                            detected_city = house_data.get('detected_city', 'æœªçŸ¥')
                            print(f"   âœ… å·²æ”¶é›†ä¸¦ä¿å­˜è³‡æ–™ ({detected_city}) - æœ¬é ç¸½è¨ˆ: {len(page_data)} ç­†")
                        else:
                            print(f"   âš ï¸  æ•¸æ“šæŠ“å–æˆåŠŸä½†ä¿å­˜å¤±æ•—ï¼Œç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹")
                    else:
                        failed_houses += 1
                        print(f"   âŒ è·³éæ­¤æˆ¿å±‹ - å¤±æ•—è¨ˆæ•¸: {failed_houses}")
                
                except Exception as e:
                    failed_houses += 1
                    print(f"   âŒ æˆ¿å±‹è™•ç†ç•°å¸¸: {e} - å¤±æ•—è¨ˆæ•¸: {failed_houses}")
                    continue  # ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹æˆ¿å±‹
                
                # æ¯çˆ¬å– 5 å€‹è©³æƒ…é ä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…è¢«å°
                if (i + 1) % 5 == 0:
                    print("çŸ­æš«ä¼‘æ¯ä¸­...")
                    time.sleep(random.uniform(2, 4))
        else:
            print(f"ç¬¬ {current_page} é æœªæ‰¾åˆ°ä»»ä½•æˆ¿å±‹é€£çµ")
            break
        
        # å˜—è©¦å°èˆªåˆ°ä¸‹ä¸€é 
        if current_page < max_pages:
            # æ§‹å»ºä¸‹ä¸€é URL
            if "?p=" in base_url:
                next_url = base_url.replace(f"?p={current_page}", f"?p={current_page + 1}")
            else:
                separator = "&" if "?" in base_url else "?"
                next_url = f"{base_url}{separator}p={current_page + 1}"
            
            print(f"å˜—è©¦è¨ªå•ä¸‹ä¸€é : {next_url}")
            if safe_get_page(browser, next_url):
                current_page += 1
                time.sleep(3)  # é é¢é–“ä¼‘æ¯æ™‚é–“å¢åŠ 
            else:
                print("ç„¡æ³•è¨ªå•ä¸‹ä¸€é ")
                has_next_page = False
        else:
            has_next_page = False
    
    print(f"\nğŸ“Š {target_region or 'ç›®æ¨™åœ°å€'} çˆ¬å–å®Œæˆ:")
    print(f"   æˆåŠŸ: {successful_houses} ç­†")
    print(f"   å¤±æ•—: {failed_houses} ç­†")
    print(f"   ç¸½è¨ˆ: {len(page_data)} ç­†æœ‰æ•ˆè³‡æ–™")
    
    if len(page_data) >= min_houses_per_region:
        print(f"   âœ… é”åˆ°ç›®æ¨™æ•¸é‡ ({min_houses_per_region} ç­†)")
    else:
        print(f"   âš ï¸  æœªé”ç›®æ¨™æ•¸é‡ ({len(page_data)}/{min_houses_per_region} ç­†)")
    
    return page_data

if __name__ == "__main__":
    # ä½¿ç”¨æ–°çš„ä¸é™åˆ¶åœ°å€çš„çˆ¬èŸ²æ–¹æ³•
    crawl_5168_all_regions()
  