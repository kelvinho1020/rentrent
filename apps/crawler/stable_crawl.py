#!/usr/bin/env python3
import sys
import os
import json
import time
import random
import traceback
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from crawler import setup_browser, safe_get_page, crawl_house_details, determine_city_from_coordinates

def stable_dual_city_crawl():
    """ç©©å®šçš„å°åŒ—å¸‚+æ–°åŒ—å¸‚çˆ¬èŸ²ï¼Œç¢ºä¿å…©å€‹åœ°å€éƒ½èƒ½å®Œæˆ"""
    browser = setup_browser()
    all_data = []
    
    try:
        # æ˜ç¢ºå®šç¾©å…©å€‹åœ°å€çš„è™•ç†é †åºï¼Œå…ˆçˆ¬æ–°åŒ—
        regions = [
            {
                "name": "æ–°åŒ—å¸‚", 
                "url": "https://rent.houseprice.tw/list/21_usage/27-26-15-23-33-28-32-36-37-34-35-31-29-30-38-39-40-41-14-13-16-20-19-21-22-18-17-24-25_zip/?p=1",
                "target_count": 40,  # æ–°åŒ—å¸‚ç›®æ¨™40ç­†
                "max_pages": 3
            },
            {
                "name": "å°åŒ—å¸‚",
                "url": "https://rent.houseprice.tw",
                "target_count": 40,  # å°åŒ—å¸‚ç›®æ¨™40ç­†
                "max_pages": 3
            }
        ]
        
        total_start_time = time.time()
        
        # ä¾åºè™•ç†æ¯å€‹åœ°å€
        for region_idx, region in enumerate(regions):
            region_name = region["name"]
            region_url = region["url"] 
            target_count = region["target_count"]
            max_pages = region["max_pages"]
            
            print(f"\n" + "="*60)
            print(f"ğŸ™ï¸ ç¬¬ {region_idx + 1}/2 å€‹åœ°å€ï¼šé–‹å§‹è™•ç† {region_name}")
            print(f"ğŸ¯ ç›®æ¨™ï¼š{target_count} ç­†è³‡æ–™ï¼Œæœ€å¤š {max_pages} é ")
            print(f"ğŸ”— URL: {region_url}")
            print("="*60)
            
            region_start_time = time.time()
            region_data = []
            
            try:
                # è¨ªå•åœ°å€é é¢
                if not safe_get_page(browser, region_url):
                    print(f"âŒ ç„¡æ³•è¨ªå• {region_name} é é¢ï¼Œè·³éæ­¤åœ°å€")
                    continue
                
                print(f"âœ… æˆåŠŸè¨ªå• {region_name} é é¢")
                
                # çˆ¬å–è©²åœ°å€çš„æ‰€æœ‰é é¢
                for page_num in range(1, max_pages + 1):
                    if len(region_data) >= target_count:
                        print(f"âœ… {region_name} å·²é”ç›®æ¨™æ•¸é‡ ({len(region_data)}/{target_count})ï¼Œåœæ­¢çˆ¬å–æ›´å¤šé é¢")
                        break
                    
                    print(f"\nğŸ“„ è™•ç† {region_name} ç¬¬ {page_num}/{max_pages} é ...")
                    
                    if page_num > 1:
                        # æ§‹å»ºä¸‹ä¸€é URL
                        if "?p=" in region_url:
                            next_url = region_url.replace("?p=1", f"?p={page_num}")
                        else:
                            separator = "&" if "?" in region_url else "?"
                            next_url = f"{region_url}{separator}p={page_num}"
                        
                        print(f"ğŸ”— è¨ªå•ç¬¬ {page_num} é : {next_url}")
                        if not safe_get_page(browser, next_url):
                            print(f"âŒ ç„¡æ³•è¨ªå•ç¬¬ {page_num} é ï¼Œåœæ­¢ç¿»é ")
                            break
                    
                    # æ”¶é›†ç•¶å‰é é¢çš„æˆ¿å±‹é€£çµ
                    house_links = browser.find_elements("css selector", "a.group")
                    print(f"ğŸ“‹ ç¬¬ {page_num} é æ‰¾åˆ° {len(house_links)} å€‹æˆ¿å±‹é€£çµ")
                    
                    if not house_links:
                        print("âš ï¸ æœ¬é ç„¡æˆ¿å±‹é€£çµï¼Œè·³é")
                        continue
                    
                    # æ”¶é›†URL
                    house_urls = []
                    for i, link in enumerate(house_links):
                        try:
                            href = link.get_attribute("href")
                            if href:
                                house_urls.append(href)
                        except Exception as e:
                            print(f"âš ï¸ æ”¶é›†ç¬¬ {i+1} å€‹é€£çµæ™‚å‡ºéŒ¯: {e}")
                    
                    print(f"ğŸ“¦ æˆåŠŸæ”¶é›† {len(house_urls)} å€‹æœ‰æ•ˆé€£çµ")
                    
                    # è™•ç†æˆ¿å±‹è©³æƒ…
                    page_processed = 0
                    for i, house_url in enumerate(house_urls):
                        if len(region_data) >= target_count:
                            print(f"âœ… {region_name} å·²é”ç›®æ¨™ï¼Œåœæ­¢è™•ç†æœ¬é å‰©é¤˜æˆ¿å±‹")
                            break
                        
                        print(f"\nğŸ  è™•ç†ç¬¬ {i+1}/{len(house_urls)} å€‹æˆ¿å±‹...")
                        
                        try:
                            house_data = crawl_house_details(browser, house_url)
                            if house_data:
                                region_data.append(house_data)
                                page_processed += 1
                                
                                city = house_data.get('city', 'æœªå–å¾—')
                                district = house_data.get('district', 'æœªå–å¾—') 
                                detected_city = house_data.get('detected_city', 'æœªå–å¾—')
                                title = house_data.get('title', 'æœªå–å¾—')[:30]
                                
                                print(f"âœ… æˆåŠŸ: {title}...")
                                print(f"   åº§æ¨™åˆ¤æ–·: {detected_city}")
                                print(f"   å°èˆªæŠ“å–: {city} {district}")
                                print(f"   {region_name} é€²åº¦: {len(region_data)}/{target_count}")
                            else:
                                print("âŒ æˆ¿å±‹è³‡æ–™æŠ“å–å¤±æ•—")
                        
                        except Exception as e:
                            print(f"âŒ è™•ç†æˆ¿å±‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        
                        # é©ç•¶ä¼‘æ¯
                        time.sleep(random.uniform(1, 2))
                    
                    print(f"ğŸ“Š ç¬¬ {page_num} é è™•ç†å®Œæˆï¼ŒæˆåŠŸ {page_processed} ç­†")
                    print(f"ğŸ“ˆ {region_name} ç´¯è¨ˆ: {len(region_data)} ç­†")
                    
                    # é é¢é–“ä¼‘æ¯
                    if page_num < max_pages:
                        time.sleep(random.uniform(2, 4))
                
                # åœ°å€è™•ç†å®Œæˆ
                region_elapsed = time.time() - region_start_time
                print(f"\nğŸ‰ {region_name} è™•ç†å®Œæˆï¼")
                print(f"ğŸ“Š æ”¶é›†è³‡æ–™: {len(region_data)} ç­†")
                print(f"â±ï¸ è€—æ™‚: {region_elapsed:.1f} ç§’")
                
                # çµ±è¨ˆè©²åœ°å€çš„è©³ç´°è³‡æ–™
                if region_data:
                    detected_cities = {}
                    nav_districts = {}
                    
                    for item in region_data:
                        # åº§æ¨™åˆ¤æ–·çµ±è¨ˆ
                        detected = item.get('detected_city', 'æœªçŸ¥')
                        detected_cities[detected] = detected_cities.get(detected, 0) + 1
                        
                        # å°èˆªæŠ“å–çµ±è¨ˆ
                        city = item.get('city', 'æœªçŸ¥')
                        district = item.get('district', 'æœªçŸ¥')
                        if city != 'æœªçŸ¥' and district != 'æœªçŸ¥':
                            key = f"{city} {district}"
                            nav_districts[key] = nav_districts.get(key, 0) + 1
                    
                    print(f"ğŸ¯ {region_name} åº§æ¨™åˆ¤æ–·çµæœ:")
                    for city, count in detected_cities.items():
                        print(f"   {city}: {count} ç­†")
                    
                    print(f"ğŸ—ºï¸ {region_name} å°èˆªæŠ“å–çµæœ:")
                    if nav_districts:
                        for district, count in nav_districts.items():
                            print(f"   {district}: {count} ç­†")
                    else:
                        print("   âŒ æœªæˆåŠŸæŠ“å–å°èˆªåœ°å€ä¿¡æ¯")
                
                # å°‡è©²åœ°å€è³‡æ–™åŠ å…¥ç¸½è³‡æ–™
                all_data.extend(region_data)
                print(f"ğŸ“ˆ ç´¯è¨ˆç¸½è³‡æ–™: {len(all_data)} ç­†")
                
            except Exception as e:
                print(f"âŒ è™•ç† {region_name} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
                traceback.print_exc()
                print(f"ğŸ”„ ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹åœ°å€...")
            
            # åœ°å€é–“ä¼‘æ¯
            if region_idx < len(regions) - 1:
                print(f"\nâ¸ï¸ {region_name} å®Œæˆï¼Œä¼‘æ¯ 5 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹åœ°å€...")
                time.sleep(5)
        
        # æ‰€æœ‰åœ°å€è™•ç†å®Œæˆ
        total_elapsed = time.time() - total_start_time
        print(f"\n" + "="*60)
        print(f"ğŸ‰ æ‰€æœ‰åœ°å€è™•ç†å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½æ”¶é›†è³‡æ–™: {len(all_data)} ç­†")
        print(f"â±ï¸ ç¸½è€—æ™‚: {total_elapsed:.1f} ç§’")
        
        # æœ€çµ‚çµ±è¨ˆ
        if all_data:
            final_city_stats = {}
            final_district_stats = {}
            
            for item in all_data:
                detected = item.get('detected_city', 'æœªçŸ¥')
                final_city_stats[detected] = final_city_stats.get(detected, 0) + 1
                
                city = item.get('city', 'æœªçŸ¥')
                district = item.get('district', 'æœªçŸ¥')
                if city != 'æœªçŸ¥' and district != 'æœªçŸ¥':
                    key = f"{city} {district}"
                    final_district_stats[key] = final_district_stats.get(key, 0) + 1
            
            print(f"\nğŸ™ï¸ æœ€çµ‚åŸå¸‚åˆ†å¸ƒ:")
            for city, count in sorted(final_city_stats.items(), key=lambda x: x[1], reverse=True):
                print(f"   {city}: {count} ç­†")
            
            print(f"\nğŸ—ºï¸ æœ€çµ‚åœ°å€åˆ†å¸ƒ:")
            if final_district_stats:
                for district, count in sorted(final_district_stats.items(), key=lambda x: x[1], reverse=True):
                    print(f"   {district}: {count} ç­†")
            else:
                print("   âŒ æœªå–å¾—è©³ç´°åœ°å€è³‡è¨Š")
            
            # ä¿å­˜è³‡æ–™
            data_file = os.path.join("data", "stable_crawl_result.json")
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ è³‡æ–™å·²ä¿å­˜è‡³: {data_file}")
        
        return all_data
        
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²åŸ·è¡Œéç¨‹ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        return []
    
    finally:
        try:
            browser.quit()
            print("ğŸ”’ ç€è¦½å™¨å·²é—œé–‰")
        except:
            pass

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ç©©å®šé›™åŸå¸‚çˆ¬èŸ²...")
    print("ğŸ“‹ å°‡ä¾åºè™•ç†: æ–°åŒ—å¸‚ â†’ å°åŒ—å¸‚")
    print("ğŸ¯ ç¢ºä¿ä¸æœƒä¸­é€”åœæ­¢")
    print("ğŸ—ºï¸ åŒ…å«è©³ç´°åœ°å€è³‡è¨ŠæŠ“å–")
    print("\né–‹å§‹åŸ·è¡Œ...")
    
    result = stable_dual_city_crawl()
    
    if result:
        print(f"\nâœ… çˆ¬èŸ²æˆåŠŸå®Œæˆï¼ç¸½å…±æ”¶é›† {len(result)} ç­†è³‡æ–™")
    else:
        print(f"\nâŒ çˆ¬èŸ²æœªæ”¶é›†åˆ°è³‡æ–™æˆ–ç™¼ç”ŸéŒ¯èª¤") 