#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°åŒ—å¸‚å°ˆç”¨çˆ¬èŸ² - å¿«é€Ÿç²å–æ–°åŒ—å¸‚ç§Ÿå±‹è³‡æ–™
"""
import time
import json
import os
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import traceback

def setup_browser():
    """è¨­ç½®ç€è¦½å™¨"""
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    browser = webdriver.Chrome(options=chrome_options)
    browser.set_page_load_timeout(60)
    browser.implicitly_wait(5)
    return browser

def determine_city_from_coordinates(lat, lng):
    """æ ¹æ“šç¶“ç·¯åº¦åˆ¤æ–·åŸå¸‚"""
    if lat is None or lng is None:
        return "æœªçŸ¥"
    
    # å°åŒ—å¸‚é‚Šç•Œ
    if 24.95 <= lat <= 25.30 and 121.45 <= lng <= 121.65:
        return "å°åŒ—å¸‚"
    
    # æ–°åŒ—å¸‚é‚Šç•Œï¼ˆæ›´å¯¬é¬†ï¼‰
    if 24.60 <= lat <= 25.40 and 121.20 <= lng <= 122.00:
        return "æ–°åŒ—å¸‚"
    
    return "å…¶ä»–"

def extract_coordinates_from_url(url):
    """å¾URLä¸­æå–åº§æ¨™"""
    try:
        import re
        
        # å¸¸è¦‹çš„åº§æ¨™æ¨¡å¼
        patterns = [
            r'@(-?\d+\.?\d*),(-?\d+\.?\d*)',  # Google Mapsæ ¼å¼
            r'lat=(-?\d+\.?\d*).*lng=(-?\d+\.?\d*)',
            r'latitude=(-?\d+\.?\d*).*longitude=(-?\d+\.?\d*)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                lat, lng = float(match.group(1)), float(match.group(2))
                return lat, lng
    except:
        pass
    return None, None

def crawl_house_details(browser, house_url):
    """çˆ¬å–å–®å€‹æˆ¿å±‹è©³æƒ…"""
    try:
        print(f"è¨ªå•: {house_url}")
        browser.get(house_url)
        time.sleep(random.uniform(2, 4))
        
        house_data = {"url": house_url}
        
        # æ¨™é¡Œ
        try:
            title_element = browser.find_element(By.CSS_SELECTOR, "h1")
            house_data["title"] = title_element.text.strip()
        except:
            house_data["title"] = "æœªçŸ¥æ¨™é¡Œ"
        
        # åƒ¹æ ¼
        try:
            price_elements = browser.find_elements(By.CSS_SELECTOR, "span")
            for element in price_elements:
                text = element.text.strip()
                if "å…ƒ/æœˆ" in text and text.replace(",", "").replace("å…ƒ/æœˆ", "").isdigit():
                    house_data["price"] = text.replace("å…ƒ/æœˆ", "").replace(",", "")
                    break
            else:
                house_data["price"] = "0"
        except:
            house_data["price"] = "0"
        
        # å°‹æ‰¾Google Mapsé€£çµç²å–åº§æ¨™
        try:
            map_links = browser.find_elements(By.CSS_SELECTOR, "a[href*='maps.google']")
            if not map_links:
                map_links = browser.find_elements(By.CSS_SELECTOR, "a[href*='google.com/maps']")
            
            if map_links:
                map_url = map_links[0].get_attribute("href")
                lat, lng = extract_coordinates_from_url(map_url)
                if lat and lng:
                    house_data["latitude"] = lat
                    house_data["longitude"] = lng
                    house_data["detected_city"] = determine_city_from_coordinates(lat, lng)
                    print(f"  âœ… åº§æ¨™: {lat}, {lng} ({house_data['detected_city']})")
                else:
                    house_data["latitude"] = None
                    house_data["longitude"] = None
                    house_data["detected_city"] = "æœªçŸ¥"
            else:
                house_data["latitude"] = None
                house_data["longitude"] = None
                house_data["detected_city"] = "æœªçŸ¥"
        except Exception as e:
            print(f"  âš ï¸ åº§æ¨™æå–å¤±æ•—: {e}")
            house_data["latitude"] = None
            house_data["longitude"] = None
            house_data["detected_city"] = "æœªçŸ¥"
        
        # å…¶ä»–æ¬„ä½è¨­ç‚ºé è¨­å€¼
        house_data.update({
            "address": house_data["title"],
            "size_ping": None,
            "house_type": None,
            "room_type": None,
            "image_urls": [],
            "contact_name": None,
            "contact_phone": None,
            "floor": None,
            "total_floor": None,
            "last_updated": None
        })
        
        return house_data
        
    except Exception as e:
        print(f"  âŒ éŒ¯èª¤: {e}")
        return None

def crawl_new_taipei():
    """çˆ¬å–æ–°åŒ—å¸‚è³‡æ–™"""
    print("ğŸ™ï¸ é–‹å§‹çˆ¬å–æ–°åŒ—å¸‚ç§Ÿå±‹è³‡æ–™...")
    
    browser = setup_browser()
    new_taipei_data = []
    
    try:
        # æ–°åŒ—å¸‚URL
        url = "https://rent.houseprice.tw/list/21_usage/27-26-15-23-33-28-32-36-37-34-35-31-29-30-38-39-40-41-14-13-16-20-19-21-22-18-17-24-25_zip/?p=1"
        print(f"è¨ªå•æ–°åŒ—å¸‚é é¢: {url}")
        
        browser.get(url)
        time.sleep(5)
        
        # æª¢æŸ¥é é¢å…§å®¹
        if "æ–°åŒ—" in browser.page_source:
            print("âœ… é é¢åŒ…å«æ–°åŒ—å¸‚å…§å®¹")
        else:
            print("âš ï¸ é é¢å¯èƒ½ä¸åŒ…å«æ–°åŒ—å¸‚å…§å®¹")
        
        # å°‹æ‰¾æˆ¿å±‹é€£çµ
        house_links = browser.find_elements(By.CSS_SELECTOR, "a.group")
        if not house_links:
            house_links = browser.find_elements(By.CSS_SELECTOR, "a[href*='/house/']")
        
        print(f"æ‰¾åˆ° {len(house_links)} å€‹æˆ¿å±‹é€£çµ")
        
        if house_links:
            # åªè™•ç†å‰5å€‹ï¼Œå¿«é€Ÿæ¸¬è©¦
            limited_links = house_links[:5]
            house_urls = []
            
            for link in limited_links:
                try:
                    href = link.get_attribute("href")
                    if href and "/house/" in href:
                        house_urls.append(href)
                except:
                    continue
            
            print(f"æœ‰æ•ˆé€£çµ: {len(house_urls)} å€‹")
            
            # çˆ¬å–æˆ¿å±‹è©³æƒ…
            for i, house_url in enumerate(house_urls):
                print(f"\nè™•ç†ç¬¬ {i+1}/{len(house_urls)} å€‹æˆ¿å±‹...")
                house_data = crawl_house_details(browser, house_url)
                
                if house_data:
                    new_taipei_data.append(house_data)
                    print(f"  âœ… æˆåŠŸæ”¶é›†ï¼š{house_data.get('title', 'æœªçŸ¥')[:30]} ({house_data.get('detected_city', 'æœªçŸ¥')})")
                
                time.sleep(random.uniform(1, 3))
        else:
            print("âŒ æœªæ‰¾åˆ°æˆ¿å±‹é€£çµ")
    
    except Exception as e:
        print(f"âŒ çˆ¬å–éç¨‹å‡ºéŒ¯: {e}")
        traceback.print_exc()
    
    finally:
        browser.quit()
    
    return new_taipei_data

def merge_with_existing_data(new_data):
    """å°‡æ–°è³‡æ–™åˆä½µåˆ°ç¾æœ‰è³‡æ–™æª”æ¡ˆ"""
    data_file = "data/data.json"
    
    try:
        # è®€å–ç¾æœ‰è³‡æ–™
        if os.path.exists(data_file):
            with open(data_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        
        print(f"ç¾æœ‰è³‡æ–™: {len(existing_data)} ç­†")
        print(f"æ–°å¢è³‡æ–™: {len(new_data)} ç­†")
        
        # åˆä½µè³‡æ–™ï¼ˆé¿å…é‡è¤‡ï¼‰
        existing_urls = {item.get('url') for item in existing_data if item.get('url')}
        added_count = 0
        
        for item in new_data:
            if item.get('url') not in existing_urls:
                existing_data.append(item)
                added_count += 1
        
        # ä¿å­˜åˆä½µå¾Œçš„è³‡æ–™
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {added_count} ç­†æ–°è³‡æ–™")
        print(f"ç¸½è¨ˆè³‡æ–™: {len(existing_data)} ç­†")
        
        # çµ±è¨ˆåœ°å€åˆ†å¸ƒ
        cities = {}
        for item in existing_data:
            city = item.get('detected_city', 'æœªçŸ¥')
            cities[city] = cities.get(city, 0) + 1
        
        print("\nğŸ“Š æœ€çµ‚åœ°å€åˆ†å¸ƒ:")
        for city, count in sorted(cities.items(), key=lambda x: x[1], reverse=True):
            print(f"  {city}: {count} ç­†")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆä½µè³‡æ–™æ™‚å‡ºéŒ¯: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ æ–°åŒ—å¸‚å°ˆç”¨çˆ¬èŸ²å•Ÿå‹•...")
    
    # çˆ¬å–æ–°åŒ—å¸‚è³‡æ–™
    new_taipei_data = crawl_new_taipei()
    
    if new_taipei_data:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(new_taipei_data)} ç­†æ–°åŒ—å¸‚è³‡æ–™")
        
        # åˆä½µåˆ°ç¾æœ‰è³‡æ–™
        if merge_with_existing_data(new_taipei_data):
            print("ğŸ‰ æ–°åŒ—å¸‚è³‡æ–™å·²æˆåŠŸæ·»åŠ åˆ°è³‡æ–™åº«ï¼")
        else:
            print("âŒ åˆä½µè³‡æ–™å¤±æ•—")
    else:
        print("âŒ æœªç²å–åˆ°æ–°åŒ—å¸‚è³‡æ–™") 